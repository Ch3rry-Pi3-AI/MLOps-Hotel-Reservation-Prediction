# ğŸ¤– **Model Training â€” MLOps Hotel Reservation Prediction**

This branch turns the **preprocessed datasets** into a **trained, versioned model** with experiment tracking via **MLflow**.
Crucially, this stage was made **much easier** thanks to the insights and validated transformations from the earlier **notebook experimentation** stage.

The goal here is to **train, tune, evaluate, and persist** a LightGBM classifier in a **reproducible, configurable** wayâ€”ready for downstream inference and deployment.

## ğŸ§¾ **Whatâ€™s New in This Stage**

* ğŸ†• **`src/model_training.py`** â€” end-to-end training pipeline (load â†’ tune â†’ evaluate â†’ save â†’ log to MLflow).
* ğŸ†• **`config/model_params.py`** â€” tidy LightGBM search space and `RandomizedSearchCV` settings.
* ğŸ”§ **`config/paths_config.py`** â€” extended with `MODELS_DIR` and `MODEL_OUTPUT_PATH`.
* ğŸ–¼ï¸ **Images for docs:** `img/model_training/mlflow_experiment.png`, `img/model_training/mlflow_run.png`.
* ğŸ“¦ **New output folder:** `artifacts/models/` â€” contains the saved model `lgbm_model.pkl`.

## ğŸ§© **Key Functionalities**

The training pipeline provides:

1. **Data loading & split** â€” expects `processed_train.csv` and `processed_test.csv` (from the previous stage).
2. **Hyperparameter search** â€” `RandomizedSearchCV` over a **LightGBM** search space.
3. **Evaluation** â€” Accuracy, Precision, Recall, and F1 reported on the held-out test set.
4. **Model persistence** â€” stores the best estimator at `artifacts/models/lgbm_model.pkl`.
5. **Experiment tracking** â€” datasets, params, metrics, and model artefacts logged to **MLflow**.

## ğŸ§  **How to Run**

### 1) Train the model

```bash
python src/model_training.py
```

### 2) View experiments in MLflow

```bash
mlflow ui --host 127.0.0.1 --port 5555
```

Then open: [http://127.0.0.1:5555](http://127.0.0.1:5555)

<p align="center">
  <img src="img/model_training/mlflow_experiment.png" alt="MLflow Experiment List" width="720" />
</p>

<p align="center">
  <img src="img/model_training/mlflow_run.png" alt="MLflow Single Run Details" width="720" />
</p>

## ğŸ—‚ï¸ **Updated Project Structure**

Only additions/updates from the previous stage are shown with markers.

```
mlops-hotel-reservation-prediction/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ processed_train.csv
â”‚   â”‚   â””â”€â”€ processed_test.csv
â”‚   â””â”€â”€ models/                         # ğŸ†• model artefacts
â”‚       â””â”€â”€ lgbm_model.pkl
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ paths_config.py                 # ğŸ”§ added MODEL_OUTPUT_PATH / models dir
â”‚   â””â”€â”€ model_params.py                 # ğŸ†• LightGBM + RandomizedSearch params
â”œâ”€â”€ img/
â”‚   â””â”€â”€ model_training/                 # ğŸ†• documentation images
â”‚       â”œâ”€â”€ mlflow_experiment.png
â”‚       â””â”€â”€ mlflow_run.png
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ model_training.py               # ğŸ†• training pipeline
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ common_functions.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ notebook.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md                           # ğŸ“– you are here
```

## ğŸ” **Pipeline Highlights**

* **Config-driven:** model search space and CV settings live in `config/model_params.py`.
* **Consistent paths:** all inputs/outputs resolved via `config/paths_config.py`.
* **Reproducible runs:** MLflow captures **datasets**, **parameters**, **metrics**, and the **model artefact**.
* **Balanced training data:** assumes the prior stage produced balanced, feature-selected datasets.


## ğŸš€ **Whatâ€™s Next â€” Training Pipeline Automation**

The next branch will focus on **building a modular training pipeline**, integrating the model training process into a repeatable, automated workflow.
This stage will introduce **structured pipeline orchestration**, enabling scheduled retraining, experiment reproducibility, and seamless handoff into CI/CD systems.

It will combine the preprocessing and model training components into a unified, end-to-end pipeline â€” the foundation for scalable **MLOps automation**.
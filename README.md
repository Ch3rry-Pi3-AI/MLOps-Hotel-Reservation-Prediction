Perfect ğŸ‘ â€” hereâ€™s the **fully updated README** for your **Data Preprocessing stage**, now including the â€œHow to Runâ€ section and preserving your established formatting and tone.

---

# âš™ï¸ **Data Preprocessing â€” MLOps Hotel Reservation Prediction**

This branch marks the transition from **notebook experimentation** to a **modular, reproducible data preprocessing pipeline**.
Following the insights gained during the **Exploratory Analysis** stage, the data scientistâ€™s workflow has now been **refactored into Python scripts** that can be reused, parameterised, and integrated into the full MLOps pipeline.

The goal of this stage is to **automate the cleaning, encoding, transformation, and feature selection process** used in the notebook â€” establishing a solid, repeatable foundation for model training in the next stage.

## ğŸ§¾ **Whatâ€™s New in This Stage**

This branch introduces several key updates and new modules:

* ğŸ†• **`src/data_preprocessing.py`** â€” a fully modular script encapsulating all data cleaning, encoding, class balancing, and feature selection logic within a single `DataProcessor` class.
  This design was made possible thanks to experimentation and validation in the previous notebook stage.
* ğŸ”§ **`config/config.yaml`** â€” updated to include configurable parameters for categorical/numerical columns, skewness threshold, and number of features to select.
* ğŸ—ºï¸ **`config/paths_config.py`** â€” updated to define new paths for processed outputs (`PROCESSED_TRAIN_DATA_PATH`, `PROCESSED_TEST_DATA_PATH`).
* ğŸ§° **`utils/common_functions.py`** â€” extended with helper utilities such as `read_yaml()` and `load_data()` for consistent data access across modules.
* ğŸ“¦ **New output folder:** `artifacts/processed/` â€” automatically created by the preprocessing pipeline to store `processed_train.csv` and `processed_test.csv`.

## ğŸ§© **Key Functionalities**

The new `DataProcessor` class performs the following steps end-to-end:

1. **Data Cleaning** â€” drops unnecessary columns (`Unnamed: 0`, `Booking_ID`) and duplicates.
2. **Categorical Encoding** â€” applies label encoding to categorical columns defined in YAML.
3. **Skewness Handling** â€” uses `np.log1p()` to transform highly skewed numeric features.
4. **Class Balancing** â€” applies **SMOTE** to mitigate booking status imbalance.
5. **Feature Selection** â€” selects top-N important features using a `RandomForestClassifier`.
6. **Data Saving** â€” writes the cleaned and balanced datasets to the new `artifacts/processed/` directory.

Each transformation step is logged using the centralised project logger and wrapped with a custom exception handler for traceable debugging.

## ğŸ§  **How to Run the Pipeline**

After activating your virtual environment and installing dependencies:

```bash
python src/data_preprocessing.py
```

This command executes the full preprocessing workflow:

* Loads raw training and test data from `artifacts/raw/`
* Applies cleaning, encoding, balancing, and feature selection
* Saves processed outputs to `artifacts/processed/processed_train.csv` and `processed_test.csv`

## ğŸ—‚ï¸ **Updated Project Structure**

```
mlops-hotel-reservation-prediction/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/                             # From previous ingestion stage
â”‚   â”‚   â”œâ”€â”€ raw.csv
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â””â”€â”€ processed/                       # ğŸ†• Newly created by this stage
â”‚       â”œâ”€â”€ processed_train.csv
â”‚       â””â”€â”€ processed_test.csv
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml                      # ğŸ”§ Updated with preprocessing params
â”‚   â””â”€â”€ paths_config.py                  # ğŸ”§ Updated with processed paths
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_preprocessing.py            # ğŸ†• Main preprocessing pipeline module
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ common_functions.py              # ğŸ”§ Extended helper functions
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ notebook.ipynb                   # From previous EDA stage
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md                            # ğŸ“– You are here
```

## ğŸ” **Pipeline Highlights**

Within `src/data_preprocessing.py`, the `DataProcessor` class:

* Loads raw CSVs from `artifacts/raw/`
* Applies consistent, YAML-driven transformations
* Balances and filters features automatically
* Saves the processed outputs ready for model training

The pipeline ensures **reproducibility**, **traceability**, and **config-driven control**, setting the foundation for scalable MLOps automation.

## ğŸš€ **Next Stage â€” Model Training**

In the next branch, the project evolves into a **Model Training** stage, where the processed data will feed into a modular training pipeline that:

* Loads preprocessed data from `artifacts/processed/`
* Trains, evaluates, and saves machine learning models
* Logs experiments to **MLflow** for versioning and reproducibility
* Prepares models for downstream **inference and deployment**

This stage completes the transformation from **raw data â†’ clean, ready-to-train datasets**, paving the way for **automated model experimentation and evaluation** in the next phase.
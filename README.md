# ðŸ§  **Exploratory Analysis â€” MLOps Hotel Reservation Prediction**

This branch represents the **data scientistâ€™s experimental stage**, where the **Hotel Reservations dataset** (previously ingested from Google Cloud Storage) is explored, cleaned, and analysed within a **Jupyter Notebook** environment.

The purpose of this stage is to **understand the data**, perform **EDA and preprocessing experiments**, and identify **key modelling strategies** â€” before the workflow is modularised and automated by an ML engineer in the next stage.



## ðŸ§¾ **What This Stage Includes**

* âœ… Jupyter Notebook (`notebooks/notebook.ipynb`) for interactive exploration
* âœ… Initial data validation (missing values, duplicates, column inspection)
* âœ… Exploratory data analysis (univariate, bivariate, and correlation)
* âœ… Preprocessing (encoding, transformations, feature checks)
* âœ… Class balancing using **SMOTE**
* âœ… Baseline modelling with multiple ML algorithms
* âœ… Random Forest feature importance and quick hyperparameter tuning
* âœ… Final model persistence (`random_forest.pkl`) for downstream use

This notebook acts as a **sandbox** for a data scientist â€” a place to experiment freely before refactoring the workflow into modular scripts and pipeline stages.



## ðŸ—‚ï¸ **Updated Project Structure**

```
mlops-hotel-reservation-prediction/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/                        # From previous data ingestion stage
â”‚       â”œâ”€â”€ raw.csv
â”‚       â”œâ”€â”€ train.csv
â”‚       â””â”€â”€ test.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ notebook.ipynb              # ðŸ” Data scientist EDA & experimentation
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ paths_config.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ common_functions.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md                       # ðŸ“– You are here
```

> ðŸ’¡ The notebook uses `artifacts/raw/train.csv` and `test.csv` generated in the previous **Data Ingestion from GCP** stage.



## ðŸ§© **Notebook Highlights**

Within `notebooks/notebook.ipynb`, youâ€™ll find clearly documented sections covering:

1. **Setup & Imports** â€” all required libraries configured for reproducibility
2. **Data Loading** â€” reads from `artifacts/raw/train.csv`
3. **Exploratory Data Analysis (EDA)** â€” distributions, correlations, relationships
4. **Preprocessing & Encoding** â€” label encoding, skewness correction, VIF check
5. **Class Balancing** â€” SMOTE applied to address booking imbalance
6. **Modelling Experiments** â€” Random Forest, Gradient Boosting, Logistic Regression, etc.
7. **Hyperparameter Tuning** â€” lightweight `RandomizedSearchCV` example
8. **Model Persistence** â€” saves `random_forest.pkl` for inference or modular pipelines



## ðŸš€ **Next Stage â€” Modular Data Processing**

In the next branch, the **data scientistâ€™s experimental workflow** will be translated into a **modular, production-ready data processing pipeline**:

* Code moved from notebook â†’ `src/data_processing/` modules
* Configurable parameters defined in YAML
* Unit tests and CI integration added
* Integration with MLflow and model registry prepared

This marks the transition from **exploration â†’ engineering** â€” bridging experimentation and full MLOps automation.
